[{"authors":null,"categories":null,"content":"Hello! I am a current M.Eng student in Electrical Engineering and Computer Science at MIT conducting research on scalable, adaptive, and efficient wireless tactile sensing. I am interested in developing intelligent wearable systems using textile-integrated electronics, low-power wireless communication protocols, and multimodal signal processing and machine learning. On this page you can find past projects I’ve worked on, as well as my current resume. Feel free to contact me at devinmur@mit.edu about anything you find here!\n","date":1732406400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1732406400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"2017-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hello! I am a current M.Eng student in Electrical Engineering and Computer Science at MIT conducting research on scalable, adaptive, and efficient wireless tactile sensing. I am interested in developing intelligent wearable systems using textile-integrated electronics, low-power wireless communication protocols, and multimodal signal processing and machine learning.","tags":null,"title":"Devin Murphy","type":"authors"},{"authors":["Devin Murphy"],"categories":null,"content":"","date":1732406400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1732406400,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://example.com/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"WiReSens Toolkit provides open-source hardware and software libraries to configure multi-sender, power-efficient, and adaptive wireless tactile sensing systems in as fast as ten minutes.","tags":["Wireless","Sensing"],"title":"WiReSens Toolkit: An Open-source Platform towards Accessible Wireless Tactile Sensing","type":"publication"},{"authors":[],"categories":null,"content":"Grateful for the opportunity to share my work on resistive tactile sensing as part of the 2024 Cambridge Science Festival!\n","date":172755e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":172755e4,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://example.com/talk/electric-skin-wearable-tech-and-the-future-of-fashion/","publishdate":"2024-09-28T19:00:00Z","relpermalink":"/talk/electric-skin-wearable-tech-and-the-future-of-fashion/","section":"event","summary":"Presenting research on wireless digitally embroidered tactile sensors at the 2024 Cambridge Science Festival","tags":[],"title":"Electric Skin: Wearable Tech and the Future of Fashion","type":"event"},{"authors":null,"categories":null,"content":"Mobile calendaring platforms provide unparalleled convenience and accessibility, yet often lack the tactile engagement and personalized touch of physical mediums. This paper introduces CalPal, an innovative multimodal digital wall calendar designed to bridge this gap by blending digital convenience with the benefits of traditional calendars. CalPal integrates pen-based event tracking, seamless synchronization with Google Calendar, intuitive gesture-based navigation, and dynamic theming based on monthly events, distinguishing it from existing digital wall calendar solutions. Our system on average is able to detect the navigation gestures (flipping forward and flipping backward) with 98% and 87% accuracy respectively. Through a comprehensive user study, we also demonstrate user satisfaction with CalPal’s intuitive interaction modalities and custom theming. Overall, these results indicate that CalPal represents a promising approach to harmonizing the strengths of digital and traditional calendaring practices, thereby providing users with a more holistic and insightful scheduling tool.\nSystem Demo Project Report If you’re interested in learning more, lots of the details are available in our project report writeup linked above!\n","date":1714521600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714521600,"objectID":"7584d892fe6037312d26b8ee4dc3e62f","permalink":"https://example.com/project/calpal/","publishdate":"2024-05-01T00:00:00Z","relpermalink":"/project/calpal/","section":"project","summary":"An intelligent multimodal digital wall calendar","tags":["Web Development","Stable Diffusion","User Interfaces"],"title":"CalPal","type":"project"},{"authors":null,"categories":null,"content":"Speech production relies on the coordinated action of the vocal cords and the movement of the vocal tract to generate a variety of sounds. The vocal tract transfer function can be used to characterize the acoustic properties of unvoiced and voiced sounds, but most techniques for estimating this function rely solely on audio signals.\nFor my final project in “Advances in Computer Vision”, I investigated the feasibility of using computer vision techniques to accurately predict the vocal tract transer function coefficients from vocal tract MRI images. I constructed a dataset by employing linear predictive coding to map 2D Sagittal-view MRI frames to vocal tract transfer function coefficients. Subsequently, I developed and evaluate a CNN-RNN architecture trained on these MRI frame and coefficient vector pairs. While my architecture tends to predict the mean coefficients of the datasets, I demonstrate the potential for added generalization capabilities provided by a combined CNN-RNN architecture, as well as the ability to learn meaningful representations for understanding speech production mechanisms through gradCAM activation visualizations.\nOverview of methods used to predict vocal tract transfer function from 2D vocal tract MRI video.Video frames and audio are separated, and Linear prediction (LP) coefficients are obtained on the audio signal associated with each frame. A Resnet50 CNN is then fine tuned for a linear regression task on the LP coefficients using the normalized MRI video frames as inputs. This tuned CNN is then used to train a CNN-LSTM model for higher accuracy prediction of the LP coefficients, which can then be used to resynthesize the original audio signal associated with a set of MRI image frames I implemented a CNN-RNN architecture for a regression task on estimated vocal tract transfer function, leveraging MRI images as inputs. The MRI images and vocal tract transfer functions were obtained through the processing of a multi-speaker dataset of real-time speech production MRI video. Initially, I fine-tuned a ResNet50 architecture to predict the vocal tract transfer function coefficients cor responding to each frame, thereby learning a 256-element embedding of MRI image features. Subsequently, a recur rent neural network with LSTM layers was employed. Its primary objective was to ingest a sequence of MRI frames and capture temporal patterns in the features derived from our pretrained CNN, enabling the prediction of LP coeffi cients for the last time step in the sequence.\nResults and Evaluation While initial visual inspection of predicted frequency responses looked promising, upon closer inspection it became clear that the model was learning and predicting the mean of the dataset.\nFrequency responses for predicted and ground truth LP coefficient vectors. The plot on the left shows an occasion where our prediction happens to match the label quite well, while the right plot hints at model prediction of the mean The means and variances of the predicted coefficients as compared to the actual coefficients further confirmed this suspicion.\nWhen plotting gradCAM activations for the 19 different coefficients, the model seemed to be using anatomically significant regions of images for prediction, for example the throat, soft palate, chin, and lips/nose. This indicates that future work could develop interpretable models which provide valuable insight into the anatomical features crucial for speech production.\nGradCam activations highlighting the lips, nose, chin, throat, and softpalate for coefficients 1, 3, 4, and 8 ","date":1711929600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711929600,"objectID":"84f1e45956567ddc27e80b1e65919a4e","permalink":"https://example.com/project/speechformants/","publishdate":"2024-04-01T00:00:00Z","relpermalink":"/project/speechformants/","section":"project","summary":"Exploring the combination of a convolutional neural network with a recurrent neural network to predict acoustic properties of the vocal tract","tags":["Deep Learning","Signal Processing"],"title":"Vocal Tract MRI to Speech","type":"project"},{"authors":null,"categories":null,"content":"Membranas is a spatial sound sculpture meant to facilitate more than human exchanges.I worked on this project in collaboration with Nicole L’Huillier for her thesis project at the MIT Media Lab. With this work we aimed to explore sounds and vibrations as media for stimulating collectivity, embodied learning, and interconnectivity well beyond the human.\nWhen I started working with Nicole on this project, we talked about using an element of the sculpture to record the wind and present it in a sonically enhanced yet authentic way. Wind sort of takes on the sound of whatever its interacting with, and it seemed like the perfect medium to help us facilitate a conversation between people and the environment. It was important to us that this communication occured in the wind’s language, that is, in a way that felt true to the wind’s voice.\nNicole made an initial attempt at this using contact microphones in a silicone membrane. We listened to the audio recorded using these microphones together and determined that it was picking up wind in the way lots of other microphones do: as a sort of “white noise” that doesn’t have much character or liveliness in it. Additionally, the contact microphones recorded a lot of things that weren’t the wind, like birds chirping or lawn mowers. We thought we could do better.\nI ran a spectrogram analysis and used that to filter the audio from the contact microphones in a variety of ways. I noticed that a lot of the sound we were getting from the wind was both low in frequency and amplitude. After doing some research, I found some interesting papers about accelerometers being used to record low frequency and low amplitude sound, such as that of ants marching under the ground. This matched the character of the sound we were hoping to record, so we moved forward with the accelerometer as our microphone.\nIn order to turn the accelerometer data into digital audio, I created the following data processing pipeline:\nI wrote C++ code to collect about 30 seconds worth of samples from the Accelerometer using an Arduino and the I2C communication protocol.\nIn the same C++ code, I wrote a TCP communication protocol to send the samples over WiFi to a computer for post processing.\nFinally. I wrote a Python script to estimate the sampling rate of the accelerometer, scale the received values so that they would be audible, and write the processed data to a WAV file to be sent to different speakers in the sculpture.\nI learned so so much from working on this project, but the biggest takeaways were:\nHow to make communication protocols robust by adding checks for missing data and waiting for missing accelerometer measures (that either got lost in I2C communication with the accelerometer or in TCP communication over WiFi with the computer).\nHow to interface with hardware components and microcontrollers, as we tried lots of different accelerometers and boards throughout our experiments.\nLearning about filtering methods and signal processing techniques, especially as they relate to digital audio.\nHere are some pictures/videos from the finished sound sculpture! I want to thank Nicole so much for her support on the project, and also give a shoutout to Jessie Mindel for the work she did on the sound design and routing for the system!\nYou can check out more of Nicole’s work here!\n","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"c7d41f35aeb7e99059a5e0f801c42688","permalink":"https://example.com/project/membranas/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/project/membranas/","section":"project","summary":"Creating an accelerometer based microphone and digital communication protocol for a spatial sound sculpture","tags":["Electronic Prototyping","Wireless Communication","Signal Processing"],"title":"Membranas","type":"project"},{"authors":null,"categories":null,"content":"Kaboom is an FPGA based multiplayer game based off of the game “Keep Talking an Nobody Explodes”. It was developed in tandem with Emma Griffiths and Willie Zhu as a final project for the class 6.111, Introductory Digital Systems Laboratory, which I took during my Junior year at MIT.\nInspired by the techy aesthetic of the FPGA, the objective of this game is to defuse a bomb before time runs out. There are various modules that must be disarmed, the order of which appear in a predetermined, random order and one after another on the screen. All minigame modules also have elements of randomness. When a Defuser gets a strike, they will have to replay the module they are currently on.\nFingerprint scan minigame that used the onboard temperature sensor We decided to add two modes to our game, both single player and multiplayer. In single player you race against the clock using the manual to defuse the bomb, and in multiplayer you race against another player to defuse the bomb faster using an esp32 module and websocket to connect the boards. The other player’s status will be displayed on your screen in multiplayer mode. If they are all disarmed before time runs out, then the bomb is defused. There is also a strike system. If you get three strikes (fail modules three times), then you will lose. The game is accompanied by a bomb defusal manual, which is purposely written in a non-straight forward manner to make defusing the bomb a bit more difficult.\nUnable to display PDF file. Download instead.\nThe minigames engaged with several sensors on the FPGA, including a microphone, temperature sensor, and accelerometer. Additionally, our system engaged with a speaker and VGA output (for the graphics). Here is a block diagram of all the components of the system:\nBlock diagram of the different modules making up the Kaboom game architecture I handled the design and implementation of much of the overarching game logic using Finite State Machines, as well as graphics and peripheral device interfacing for several of the minigames. Overall, this project gave me a deeper understanding of what hardware is doing when rendering graphics, which was pretty neat! Here’s the video we put together for the project:\nYou can find out more about the implementation details for the game in the report linked at the top of the article!\n","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"77227b9d80e8a155d999d118e4d35b9f","permalink":"https://example.com/project/kaboom/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/project/kaboom/","section":"project","summary":"An FPGA based cooperative puzzle game in which players work together to defuse a bomb","tags":["FPGA"],"title":"Kaboom","type":"project"},{"authors":null,"categories":null,"content":"For my group’s final project in a Machine Learning and Data Science in Politics class at MIT, we used k means clustering and some causal analysis methods to categorize campagin spending data into distinct strategies, and investigate how spending in these strategies can influence election outcomes.\nProject Motivation The motivation for this project stems from the fact that campaigns spend millions of dollars every year trying to win elections, and politicians inevitably end up spending a lot of time trying to raise funds for this expenditure.\nThis begs the question: What’s the best way to spend that money? In the US, candidates for the Senate and House who outspend their opponents win approximately 80-90% of the time. Could certain allocation decisions and spending strategies help lower-funded candidates maximize their chances of winning the election?\nData Analysis Strategy To answer this question, we first identified and characterized spending strategies through a principal component analysis, k-means clustering, and a difference of means visualization. In order to better understand the impact of each spending strategy on election outcome, we matched candidates and calculated the Average Treatment Effect (ATE) of each spending strategy. All of our data analysis was done in R.\nFor the project, we specifically focused on data for candidates for the US House of Representatives in 2010, 2012, and 2014. We selected campaign spending categories and demographic information commonly used for voting behavior analysis. A list of the datasets used can be seen in the poster presentation of this project at the end of the page.\nAfter running a principal component analysis and using the top principal components in our k-means clustering algorithm, we identified 7 distinct spending clusters.\nCandidate campaigns grouped by K Means Clustering To see what variables from our data set characterized these clusters, we ran a difference in means analysis of the spending, candidate, and district variables between each cluster and the overall data. Here is a chart displaying some of the defining characteristics for each of the identified spending clusters. Of particular interest to us were clusters 1, 5, and 6, as they seemed most distinct from the other clusters on the basis of these first two principal components.\nCharateristics of the 7 identified clusters The clusters of our data were mostly explained by 1-2 spending categories, party of the candidate, and incumbency. For example, we’d expect the candidates belonging to cluster 1 to have spent relatively more on campagin mailings and materials. For candidates in cluster 5, we’d expect more spending in campagin consulting, and more democrats spending in this area than in other spending clusters.\nCausal Analysis To determine if spending according to a strategy defined by any of the clusters that we identified would have a notable impact on the outcome of an election, we first matched candidate campaigns by a variety of variables like candidate gender, candidate age, district size, etc. We then analyzed the Average Treatment Effect (ATE) of each of the 7 identified clusters on our voteshare data.\nTo calculate the ATE, we used an OLS regression of vote share on the treatment variables for each cluster. Shown below are the 95% confidence interval for the Beta term of the linear regression for both unmatched and matched candidates by cluster.\nAverage Treatment Effects for the 7 spending clusters we identified The confidence intervals for ATE are large, and most of them touch 0. Thus, our data does not necessarily show a causal relationship between cluster classification and vote-share. However, cluster 5 does have a rather high ATE with a confidence interval not including 0. The Beta for this cluster’s regression was at 18.03. This suggests that allocating spending towards campaign consulting, which characterizes cluster 5, could possibly help a candidate win a higher percentage of votes in their election.\nThoughts and Project Poster This was a really cool project, I liked being able to work with real world data to answer a laregly-scoped question like the one we were investigating. We designed the analysis for the project, came up with a research question, found and cleaned the appropriate data, coded the analysis and presented our results. It was great having ownership over that whole process from start to finish.\nOur poster is available in the pdf link at the top of the page!\n","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"2995cadb24240c5406428caa142bb4fd","permalink":"https://example.com/project/campaign/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/project/campaign/","section":"project","summary":"Applying PCA and k-means clustering to analyze and assess the effectiveness of campaign spending strategies for U.S. House of Representatives candidates","tags":["Machine Learning"],"title":"Campaign Spending Strategies","type":"project"},{"authors":null,"categories":null,"content":"For Them is a piece for solo clarinet and live electronics that I composed using MaxMSP and sibelius. The piece is meant to be performed with a live solo clarinet, and either a pedal or person that sends a signal to a Max patch to progress the electronics through each phase of the piece (of which there are 8). In this way, the electronics become their own instrument used to accompany the clarinetist. I created the signal routing and processing pipelines for this piece in Max with the help of the odot package.\nFor this project, I really wanted to explore sonification and how data can be represented with music in a meaningful way. My piece explores the sensitive but increasingly pertinent topic of gun violence in the United States. To compose the clarinet music, I mapped the timeline of mass shootings in the United States for the past 100 years to a 33 bar piece of music. So every measure between bar 11 and bar 44 represents 3.3 years in United States history.\nI used incidents considered mass shootings by at least two of the following sources: Stanford MSA Data Project, Gun Violence Archive, Mother Jones, The Washington Post, ABC News,FBI, Congressional Research service. I repeat a somber theme over these bars, disrupting the theme with random notes from the 12 semitones. In this way, I also encapsulated some serialist composition techniques.\nThe opening and main theme of \u0026#34;For Them\u0026#34; Perturbations of the opening theme, each representing a mass shooting. I wanted to incorporate electronic techniques that would enhance these disruptions, and ended up using reverb and downsampling as the main tools for this.\nEventually it just becomes too difficult to fit all of the notes in one measure and have the music playable on clarinet. So at measure 40, which would begin to represent the last decade or so, I give the clarinet straight sixteenth notes and have an fm synthesized drunken walk accompany the clarinet. The intention of this was to create harmonic chaos that leads all the way up to the end of measure 44.\nThe opening theme also features a poem by John Keats, titled “When I Have Fears that I May Cease to Be”. I wanted to play around with turning speech into music, and when I heard this poem I thought it would be perfect to accompany the clarinet in the beginning. To do this, I first used the ~bonk object on the poem to create a binary signal stream that is “on” whenever John Keats is speaking. This signal then triggers an fm synthesis patch that picks a random harmonization of several pitches to play. These pitches are randomly chosen from a probability distribution that I have set in Max.\nPatch showing Bonk to Fm Synthesis workflow The end of the piece features a harmonization of the theme and is mostly a representation of my own frustration and exhaustion with gun violence. I tried to communicate these two emotions with a lax tempo, glissandos, trills, and the minor third harmonizations.\nGlissandos, trills, harmonizations I was incredibly grateful to have For Them performed by principal clarinetist of the Boston Philharmonic Rane Moore as a final project presentation. Here’s a recording of that performance:\nDownload WAV audio. And the clarinet sheet music can be downloaded at the pdf link above.\n","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556668800,"objectID":"b3fe0a6e3e289322ecfd0fad8acff0a9","permalink":"https://example.com/project/forthem/","publishdate":"2019-05-01T00:00:00Z","relpermalink":"/project/forthem/","section":"project","summary":"A piece I composed for solo clarinet and live electronics using Max","tags":["Signal Processing"],"title":"For Them","type":"project"}]