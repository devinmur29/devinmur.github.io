<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Devin Murphy</title><link>https://example.com/</link><atom:link href="https://example.com/index.xml" rel="self" type="application/rss+xml"/><description>Devin Murphy</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 24 Oct 2022 00:00:00 +0000</lastBuildDate><image><url>https://example.com/media/icon_hua6395a99da531b2e70e32e5401dc9674_120945_512x512_fill_lanczos_center_3.png</url><title>Devin Murphy</title><link>https://example.com/</link></image><item><title>WiReSens Toolkit: An Open-source Platform towards Accessible Wireless Tactile Sensing</title><link>https://example.com/publication/preprint/</link><pubDate>Sun, 24 Nov 2024 00:00:00 +0000</pubDate><guid>https://example.com/publication/preprint/</guid><description/></item><item><title>Electric Skin: Wearable Tech and the Future of Fashion</title><link>https://example.com/talk/electric-skin-wearable-tech-and-the-future-of-fashion/</link><pubDate>Sat, 28 Sep 2024 19:00:00 +0000</pubDate><guid>https://example.com/talk/electric-skin-wearable-tech-and-the-future-of-fashion/</guid><description>&lt;p>Grateful for the opportunity to share my work on resistive tactile sensing as part of the 2024 Cambridge Science Festival!&lt;/p>
&lt;div class="gallery-grid">
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-csf2024" href="https://example.com/media/albums/csf2024/092824_CambridgeScienceFestival_oliviamoonphotography-126.jpg" >
&lt;img src="https://example.com/media/albums/csf2024/_hud2a3dd6e2d1bc63c3ea6e1af5c8037be_8772354_676d46921932697fb815b6569ed97309.webp" loading="lazy" alt="092824_CambridgeScienceFestival_oliviamoonphotography-126.jpg" width="750" height="500">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-csf2024" href="https://example.com/media/albums/csf2024/20240928_cambridgesciencefestival_mjc_3438.jpg" >
&lt;img src="https://example.com/media/albums/csf2024/20240928_cambridgesciencefestival_mjc_3438_hu3d03a01dcc18bc5be0e67db3d8d209a6_2363859_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="20240928_cambridgesciencefestival_mjc_3438.jpg" width="750" height="500">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-csf2024" href="https://example.com/media/albums/csf2024/20240928_cambridgesciencefestival_mjc_4086.jpg" >
&lt;img src="https://example.com/media/albums/csf2024/20240928_cambridgesciencefestival_mjc_4086_hu3d03a01dcc18bc5be0e67db3d8d209a6_1040021_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="20240928_cambridgesciencefestival_mjc_4086.jpg" width="750" height="500">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-csf2024" href="https://example.com/media/albums/csf2024/IMG_9930.JPG" >
&lt;img src="https://example.com/media/albums/csf2024/IMG_9930_hud8e9d17f8ad769e88ebe8fffec3a13bb_2886616_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_9930.JPG" width="750" height="563">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-csf2024" href="https://example.com/media/albums/csf2024/IMG_9931.JPG" >
&lt;img src="https://example.com/media/albums/csf2024/IMG_9931_huce5293b30379504c5f0c3f7ae4d21cc5_2523121_750x750_fit_q75_h2_lanczos.webp" loading="lazy" alt="IMG_9931.JPG" width="750" height="563">
&lt;/a>
&lt;/div>
&lt;div class="gallery-item gallery-item--medium">
&lt;a data-fancybox="gallery-csf2024" href="https://example.com/media/albums/csf2024/newImg.gif" >
&lt;img src="https://example.com/media/albums/csf2024/newImg_hu1ebb31297f51a9643fc0b7f6609126b9_7317170_750x750_fit_q75_h2_lanczos_1.webp" loading="lazy" alt="newImg.gif" width="600" height="338">
&lt;/a>
&lt;/div>
&lt;/div>
&lt;!--
&lt;div class="alert alert-note">
&lt;div>
Click on the &lt;strong>Slides&lt;/strong> button above to view the built-in slides feature.
&lt;/div>
&lt;/div>
Slides can be added in a few ways:
- **Create** slides using Hugo Blox Builder's [_Slides_](https://docs.hugoblox.com/reference/content-types/) feature and link using `slides` parameter in the front matter of the talk file
- **Upload** an existing slide deck to `static/` and link using `url_slides` parameter in the front matter of the talk file
- **Embed** your slides (e.g. Google Slides) or presentation video on this page using [shortcodes](https://docs.hugoblox.com/reference/markdown/).
Further event details, including [page elements](https://docs.hugoblox.com/reference/markdown/) such as image galleries, can be added to the body of this page. --></description></item><item><title>CalPal</title><link>https://example.com/project/calpal/</link><pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate><guid>https://example.com/project/calpal/</guid><description>&lt;p>Mobile calendaring platforms provide unparalleled convenience and accessibility, yet often lack the tactile engagement and personalized touch of physical mediums. This paper introduces CalPal, an innovative multimodal digital wall calendar designed to bridge this gap by blending digital convenience with the benefits of traditional calendars. CalPal integrates pen-based event tracking, seamless synchronization with Google Calendar, intuitive gesture-based navigation, and dynamic theming based on monthly events, distinguishing it from existing digital wall calendar solutions. Our system on average is able to detect the navigation gestures (flipping forward and flipping backward) with 98% and 87% accuracy respectively. Through a comprehensive user study, we also demonstrate user satisfaction with CalPal&amp;rsquo;s intuitive interaction modalities and custom theming. Overall, these results indicate that CalPal represents a promising approach to harmonizing the strengths of digital and traditional calendaring practices, thereby providing users with a more holistic and insightful scheduling tool.&lt;/p>
&lt;h2 id="system-demo">System Demo&lt;/h2>
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/TuvQ1oiRmB8?si=tLA5axvZc4PYfjer" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen>&lt;/iframe>
&lt;h2 id="project-report">Project Report&lt;/h2>
&lt;p>If you&amp;rsquo;re interested in learning more, lots of the details are available in our project report writeup linked above!&lt;/p></description></item><item><title>Vocal Tract MRI to Speech</title><link>https://example.com/project/speechformants/</link><pubDate>Mon, 01 Apr 2024 00:00:00 +0000</pubDate><guid>https://example.com/project/speechformants/</guid><description>&lt;p>Speech production relies on the coordinated action of the vocal cords and the movement of the vocal tract to generate a variety of sounds. The vocal tract transfer function can be used to characterize the acoustic properties of unvoiced and voiced sounds, but most techniques for estimating this function rely solely on audio signals.&lt;/p>
&lt;p>For my final project in &amp;ldquo;Advances in Computer Vision&amp;rdquo;, I investigated the feasibility of using computer vision techniques to accurately predict the vocal tract transer function coefficients from vocal tract MRI images. I constructed a dataset by employing linear predictive coding
to map 2D Sagittal-view MRI frames to vocal tract transfer function coefficients. Subsequently, I developed and evaluate a CNN-RNN architecture trained on these MRI frame and coefficient vector pairs. While my architecture tends to predict the mean coefficients of the datasets, I demonstrate the potential for added generalization capabilities provided by a combined CNN-RNN architecture, as well as the ability to learn meaningful representations for understanding speech production mechanisms through gradCAM activation visualizations.&lt;/p>
&lt;figure>
&lt;img src="CVMethodology.png" alt="Methodology Diagram">
&lt;figcaption>Overview of methods used to predict vocal tract transfer function from 2D vocal tract MRI video.Video frames and audio are
separated, and Linear prediction (LP) coefficients are obtained on the audio signal associated with each frame. A Resnet50 CNN is then
fine tuned for a linear regression task on the LP coefficients using the normalized MRI video frames as inputs. This tuned CNN is then
used to train a CNN-LSTM model for higher accuracy prediction of the LP coefficients, which can then be used to resynthesize the original
audio signal associated with a set of MRI image frames&lt;/figcaption>
&lt;/figure>
&lt;p>I implemented a CNN-RNN architecture for a regression
task on estimated vocal tract transfer function, leveraging
MRI images as inputs. The MRI images and vocal tract transfer functions were obtained through the processing of
a multi-speaker dataset of real-time speech production MRI
video. Initially, I fine-tuned a ResNet50 architecture
to predict the vocal tract transfer function coefficients cor
responding to each frame, thereby learning a 256-element
embedding of MRI image features. Subsequently, a recur
rent neural network with LSTM layers was employed. Its
primary objective was to ingest a sequence of MRI frames
and capture temporal patterns in the features derived from
our pretrained CNN, enabling the prediction of LP coeffi
cients for the last time step in the sequence.&lt;/p>
&lt;h2 id="results-and-evaluation">Results and Evaluation&lt;/h2>
&lt;p>While initial visual inspection of predicted frequency responses looked promising, upon closer inspection it became clear that the model was learning and predicting the mean of the dataset.&lt;/p>
&lt;figure>
&lt;img src="pred1.png" alt="pred1">
&lt;img src="pred2.png" alt="pred2">
&lt;figcaption>Frequency responses for predicted and ground truth LP
coefficient vectors. The plot on the left shows an occasion where
our prediction happens to match the label quite well, while the
right plot hints at model prediction of the mean&lt;/figcaption>
&lt;/figure>
&lt;p>The means and variances of the predicted coefficients as compared to the actual coefficients further confirmed this suspicion.&lt;/p>
&lt;!-- &lt;figure>
&lt;img src="./../cvfinalproj/means.png" alt="means">
&lt;img src="./../cvfinalproj/variance.png" alt="variance">
&lt;figcaption>Means and Varaince for each of the 19 LP coefficients across outputs for the test set&lt;/figcaption>
&lt;/figure> -->
&lt;p>When plotting gradCAM activations for the 19 different coefficients, the model seemed to be using anatomically significant regions of images for prediction, for example the throat, soft palate, chin, and lips/nose. This indicates that future work could develop interpretable models which provide valuable insight into the anatomical features crucial for speech production.&lt;/p>
&lt;figure>
&lt;img src="gradcam1.png" alt="gradcam1">
&lt;img src="gradcam2.png" alt="gradcam2">
&lt;figcaption>GradCam activations highlighting the lips, nose, chin, throat, and softpalate for coefficients 1, 3, 4, and 8&lt;/figcaption>
&lt;/figure></description></item><item><title>Membranas</title><link>https://example.com/project/membranas/</link><pubDate>Sun, 01 May 2022 00:00:00 +0000</pubDate><guid>https://example.com/project/membranas/</guid><description>&lt;p>&lt;em>Membranas&lt;/em> is a spatial sound sculpture meant to facilitate more than human exchanges.I worked on this project in collaboration with Nicole L&amp;rsquo;Huillier for her thesis project at the MIT Media Lab. With this work we aimed to explore sounds and vibrations as media for stimulating collectivity, embodied learning, and interconnectivity well beyond the human.&lt;/p>
&lt;div>
&lt;img src="membrana_2.jpg" alt="Membrana 1">
&lt;/div>
&lt;p>When I started working with Nicole on this project, we talked about using an element of the sculpture to record the wind and present it in a sonically enhanced yet authentic way. Wind sort of takes on the sound of whatever its interacting with, and it seemed like the perfect medium to help us facilitate a conversation between people and the environment. It was important to us that this communication occured in the &lt;strong>&lt;em>wind&amp;rsquo;s language&lt;/em>&lt;/strong>, that is, in a way that felt true to the wind&amp;rsquo;s voice.&lt;/p>
&lt;p>Nicole made an initial attempt at this using contact microphones in a silicone membrane. We listened to the audio recorded using these microphones together and determined that it was picking up wind in the way lots of other microphones do: as a sort of &amp;ldquo;white noise&amp;rdquo; that doesn&amp;rsquo;t have much character or liveliness in it. Additionally, the contact microphones recorded a lot of things that weren&amp;rsquo;t the wind, like birds chirping or lawn mowers. We thought we could do better.&lt;/p>
&lt;p>I ran a spectrogram analysis and used that to filter the audio from the contact microphones in a variety of ways. I noticed that a lot of the sound we were getting from the wind was both low in frequency and amplitude. After doing some research, I found some interesting papers about accelerometers being used to record low frequency and low amplitude sound, such as that of ants marching under the ground. This matched the character of the sound we were hoping to record, so we moved forward with the accelerometer as our microphone.&lt;/p>
&lt;p>In order to turn the accelerometer data into digital audio, I created the following data processing pipeline:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>I wrote C++ code to collect about 30 seconds worth of samples from the Accelerometer using an Arduino and the I2C communication protocol.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>In the same C++ code, I wrote a TCP communication protocol to send the samples over WiFi to a computer for post processing.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Finally. I wrote a Python script to estimate the sampling rate of the accelerometer, scale the received values so that they would be audible, and write the processed data to a WAV file to be sent to different speakers in the sculpture.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;div>
&lt;img src="accel_process.png" alt="Accel Process">
&lt;/div>
&lt;p>I learned so so much from working on this project, but the biggest takeaways were:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>How to make communication protocols robust by adding checks for missing data and waiting for missing accelerometer measures (that either got lost in I2C communication with the accelerometer or in TCP communication over WiFi with the computer).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>How to interface with hardware components and microcontrollers, as we tried lots of different accelerometers and boards throughout our experiments.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Learning about filtering methods and signal processing techniques, especially as they relate to digital audio.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Here are some pictures/videos from the finished sound sculpture! I want to thank Nicole so much for her support on the project, and also give a shoutout to Jessie Mindel for the work she did on the sound design and routing for the system!&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://player.vimeo.com/video/818163062" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="vimeo video" webkitallowfullscreen mozallowfullscreen allowfullscreen>&lt;/iframe>
&lt;/div>
&lt;div>
&lt;img src="membrana_1.jpg" alt="Membrana 1">
&lt;/div>
&lt;p>You can check out more of Nicole&amp;rsquo;s work &lt;a href="https://nicolelhuillier.com/" target="_blank" rel="noopener">here&lt;/a>!&lt;/p></description></item><item><title>Kaboom</title><link>https://example.com/project/kaboom/</link><pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate><guid>https://example.com/project/kaboom/</guid><description>&lt;p>Kaboom is an FPGA based multiplayer game based off of the game &lt;a href="https://keeptalkinggame.com/" target="_blank" rel="noopener">&amp;ldquo;Keep Talking an Nobody Explodes&amp;rdquo;&lt;/a>. It was developed in tandem with Emma Griffiths and Willie Zhu as a final project for the class &lt;a href="https://keeptalkinggame.com/" target="_blank" rel="noopener">6.111&lt;/a>, Introductory Digital Systems Laboratory, which I took during my Junior year at MIT.&lt;/p>
&lt;p>Inspired by the techy aesthetic of the FPGA, the objective of this game is to defuse a bomb before time runs out.
There are various modules that must be disarmed, the order of which appear in a
predetermined, random order and one after another on the screen. All minigame
modules also have elements of randomness. When a Defuser gets a strike, they will have to
replay the module they are currently on.&lt;/p>
&lt;figure>
&lt;img src="fingerprint_game.png" alt="Kaboom Fingerprint Minigame">
&lt;figcaption>Fingerprint scan minigame that used the onboard temperature sensor&lt;/figcaption>
&lt;/figure>
&lt;p>We decided to add two modes to
our game, both single player and multiplayer. In single player you race against the clock using
the manual to defuse the bomb, and in multiplayer you race against another player to defuse
the bomb faster using an esp32 module and websocket to connect the boards. The other
player’s status will be displayed on your screen in multiplayer mode. If they are all disarmed before time runs out, then the bomb is defused.
There is also a strike system. If you get three strikes (fail modules three times), then you will
lose. The game is accompanied by a bomb defusal manual, which is purposely written in a
non-straight forward manner to make defusing the bomb a bit more difficult.&lt;/p>
&lt;object data="kaboom_manual.pdf" type="application/pdf" width="75%" height="500px">
&lt;p>Unable to display PDF file. &lt;a href="kaboom_manual.pdf">Download&lt;/a> instead.&lt;/p>
&lt;/object>
&lt;p>The minigames engaged with several sensors on the FPGA, including a microphone, temperature sensor, and accelerometer. Additionally, our system engaged with a speaker and VGA output (for the graphics). Here is a block diagram of all the components of the system:&lt;/p>
&lt;figure>
&lt;img src="block_diagram.png" alt="Kaboom arhcitecture block diagram">
&lt;figcaption>Block diagram of the different modules making up the Kaboom game architecture&lt;/figcaption>
&lt;/figure>
&lt;p>I handled the design and implementation of much of the overarching game logic using Finite State Machines, as well as graphics and peripheral device interfacing for several of the minigames. Overall, this project gave me a deeper understanding of what hardware is doing when rendering graphics, which was pretty neat! Here&amp;rsquo;s the video we put together for the project:&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://player.vimeo.com/video/876750178" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="vimeo video" webkitallowfullscreen mozallowfullscreen allowfullscreen>&lt;/iframe>
&lt;/div>
&lt;p>You can find out more about the implementation details for the game in the report linked at the top of the article!&lt;/p></description></item><item><title>Campaign Spending Strategies</title><link>https://example.com/project/campaign/</link><pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate><guid>https://example.com/project/campaign/</guid><description>&lt;p>For my group&amp;rsquo;s final project in a Machine Learning and Data Science in Politics class at MIT, we used k means clustering and some causal analysis methods to categorize campagin spending data into distinct strategies, and investigate how spending in these strategies can influence election outcomes.&lt;/p>
&lt;h2 id="project-motivation">Project Motivation&lt;/h2>
&lt;p>The motivation for this project stems from the fact that campaigns spend millions of dollars every year trying to win elections, and politicians inevitably end up spending a lot of time trying to raise funds for this expenditure.&lt;/p>
&lt;p>This begs the question: What&amp;rsquo;s the best way to spend that money? In the US, candidates for the Senate and House who outspend their opponents
win approximately 80-90% of the time. Could certain allocation decisions and spending strategies help lower-funded candidates maximize their chances of winning the election?&lt;/p>
&lt;h2 id="data-analysis-strategy">Data Analysis Strategy&lt;/h2>
&lt;p>To answer this question, we first identified and characterized spending strategies through a principal component analysis, k-means clustering, and a difference of means visualization. In order to better understand the impact of each spending strategy on election outcome, we matched candidates and calculated the Average Treatment Effect (ATE) of each spending strategy. All of our data analysis was done in R.&lt;/p>
&lt;p>For the project, we specifically focused on data for candidates for the US House of Representatives in 2010, 2012, and 2014. We selected campaign spending categories and demographic information commonly used for voting behavior analysis. A list of the datasets used can be seen in the poster presentation of this project at the end of the page.&lt;/p>
&lt;p>After running a principal component analysis and using the top principal components in our k-means clustering algorithm, we identified 7 distinct spending clusters.&lt;/p>
&lt;figure>
&lt;img src="pca_analysis.png" alt="Result of K Means Clustering">
&lt;figcaption>Candidate campaigns grouped by K Means Clustering&lt;/figcaption>
&lt;/figure>
&lt;p>To see what variables from our data set characterized these clusters, we ran a difference in means analysis of the spending, candidate, and district variables between each cluster and the overall data. Here is a chart displaying some of the defining characteristics for each of the identified spending clusters. Of particular interest to us were clusters 1, 5, and 6, as they seemed most distinct from the other clusters on the basis of these first two principal components.&lt;/p>
&lt;figure>
&lt;img src="cluster_chars.png" alt="Cluster Characteristics Chart">
&lt;figcaption>Charateristics of the 7 identified clusters&lt;/figcaption>
&lt;/figure>
&lt;p>The clusters of our data were mostly explained by 1-2 spending categories, party of the candidate, and incumbency. For example, we&amp;rsquo;d expect the candidates belonging to cluster 1 to have spent relatively more on campagin mailings and materials. For candidates in cluster 5, we&amp;rsquo;d expect more spending in campagin consulting, and more democrats spending in this area than in other spending clusters.&lt;/p>
&lt;h2 id="causal-analysis">Causal Analysis&lt;/h2>
&lt;p>To determine if spending according to a strategy defined by any of the clusters that we identified would have a notable impact on the outcome of an election, we first matched candidate campaigns by a variety of variables like candidate gender, candidate age, district size, etc. We then analyzed the Average Treatment Effect (ATE) of each of the 7 identified clusters on our voteshare data.&lt;/p>
&lt;p>To calculate the ATE, we used an OLS regression of vote share on the treatment variables for each cluster. Shown below are the 95% confidence interval for the Beta term of the linear regression for both unmatched and matched candidates by cluster.&lt;/p>
&lt;figure>
&lt;img src="ATEs.png" alt="Average Treatment Effect">
&lt;figcaption>Average Treatment Effects for the 7 spending clusters we identified&lt;/figcaption>
&lt;/figure>
&lt;p>The confidence intervals for ATE are large, and most of them touch 0. Thus, our data does not necessarily show a causal relationship between cluster classification and vote-share. However, cluster 5 does have a rather high ATE with a confidence interval not including 0. The Beta for this cluster’s regression was at 18.03. This suggests that allocating spending towards campaign consulting, which characterizes cluster 5, could possibly help a candidate win a higher percentage of votes in their election.&lt;/p>
&lt;h2 id="thoughts-and-project-poster">Thoughts and Project Poster&lt;/h2>
&lt;p>This was a really cool project, I liked being able to work with real world data to answer a laregly-scoped question like the one we were investigating. We designed the analysis for the project, came up with a research question, found and cleaned the appropriate data, coded the analysis and presented our results. It was great having ownership over that whole process from start to finish.&lt;/p>
&lt;p>Our poster is available in the pdf link at the top of the page!&lt;/p></description></item><item><title>For Them</title><link>https://example.com/project/forthem/</link><pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate><guid>https://example.com/project/forthem/</guid><description>&lt;p>For Them is a piece for solo clarinet and live electronics that I composed using MaxMSP and sibelius. The piece is meant to be performed with a live solo clarinet, and either a pedal or person that sends a signal to a Max patch to progress the electronics through each phase of the piece (of which there are 8). In this way, the electronics become their own instrument used to accompany the clarinetist. I created the signal routing and processing pipelines for this piece in Max with the help of the &lt;a href=https://cycling74.com/articles/cnmat-odot-tools-for-osc-and-beyond>odot package&lt;/a>.&lt;/p>
&lt;p>For this project, I really wanted to explore sonification and how data can be represented with music in a meaningful way. My piece explores the sensitive but increasingly pertinent topic of gun violence in the United States. To compose the clarinet music, I mapped the timeline of mass shootings in the United States for the past 100 years to a 33 bar piece of music. So every measure between bar 11 and bar 44 represents 3.3 years in United States history.&lt;/p>
&lt;p>I used incidents considered mass shootings by at least two of the following sources: Stanford MSA Data Project, Gun Violence Archive, Mother Jones, The Washington Post, ABC News,FBI, Congressional Research service. I repeat a somber theme over these bars, disrupting the theme with random notes from the 12 semitones. In this way, I also encapsulated some serialist composition techniques.&lt;/p>
&lt;figure>
&lt;img src="for_them_theme.png" alt="Theme">
&lt;figcaption>The opening and main theme of "For Them"&lt;/figcaption>
&lt;/figure>
&lt;figure>
&lt;img src="for_them_perturbs.png" alt="Perturbs">
&lt;figcaption>Perturbations of the opening theme, each representing a mass shooting.&lt;/figcaption>
&lt;/figure>
&lt;p>I wanted to incorporate electronic techniques that would enhance these disruptions, and ended up using reverb and downsampling as the main tools for this.&lt;/p>
&lt;p>Eventually it just becomes too difficult to fit all of the notes in one measure and have the music playable on clarinet. So at measure 40, which would begin to represent the last decade or so, I give the clarinet straight sixteenth notes and have an fm synthesized drunken walk accompany the clarinet. The intention of this was to create harmonic chaos that leads all the way up to the end of measure 44.&lt;/p>
&lt;p>The opening theme also features a poem by John Keats, titled “When I Have Fears that I May Cease to Be”. I wanted to play around with turning speech into music, and when I heard this poem I thought it would be perfect to accompany the clarinet in the beginning. To do this, I first used the ~bonk object on the poem to create a binary signal stream that is &amp;ldquo;on&amp;rdquo; whenever John Keats is speaking. This signal then triggers an fm synthesis patch that picks a random harmonization of several pitches to play. These pitches are randomly chosen from a probability distribution that I have set in Max.&lt;/p>
&lt;figure>
&lt;img src="bonk_workflow.png" alt="Bonk Workflow">
&lt;figcaption>Patch showing Bonk to Fm Synthesis workflow&lt;/figcaption>
&lt;/figure>
&lt;p>The end of the piece features a harmonization of the theme and is mostly a representation of my own frustration and exhaustion with gun violence. I tried to communicate these two emotions with a lax tempo, glissandos, trills, and the minor third harmonizations.&lt;/p>
&lt;figure>
&lt;img src="for_them_trem.png" alt="Harmonizations and Tremolos">
&lt;figcaption>Glissandos, trills, harmonizations&lt;/figcaption>
&lt;/figure>
&lt;p>I was incredibly grateful to have For Them performed by principal clarinetist of the Boston Philharmonic Rane Moore as a final project presentation. Here&amp;rsquo;s a recording of that performance:&lt;/p>
&lt;audio controls>
&lt;source src="for_them_audio.wav" type="audio/wav" />
&lt;p>
Download &lt;a href="for_them_audio.wav">WAV&lt;/a> audio.
&lt;/p>
&lt;/audio>
&lt;p>And the clarinet sheet music can be downloaded at the pdf link above.&lt;/p></description></item></channel></rss>