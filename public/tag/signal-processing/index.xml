<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Signal Processing | Devin Murphy</title><link>https://example.com/tag/signal-processing/</link><atom:link href="https://example.com/tag/signal-processing/index.xml" rel="self" type="application/rss+xml"/><description>Signal Processing</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 01 Apr 2024 00:00:00 +0000</lastBuildDate><image><url>https://example.com/media/icon_hua6395a99da531b2e70e32e5401dc9674_120945_512x512_fill_lanczos_center_3.png</url><title>Signal Processing</title><link>https://example.com/tag/signal-processing/</link></image><item><title>Vocal Tract MRI to Speech</title><link>https://example.com/project/speechformants/</link><pubDate>Mon, 01 Apr 2024 00:00:00 +0000</pubDate><guid>https://example.com/project/speechformants/</guid><description>&lt;p>Speech production relies on the coordinated action of the vocal cords and the movement of the vocal tract to generate a variety of sounds. The vocal tract transfer function can be used to characterize the acoustic properties of unvoiced and voiced sounds, but most techniques for estimating this function rely solely on audio signals.&lt;/p>
&lt;p>For my final project in &amp;ldquo;Advances in Computer Vision&amp;rdquo;, I investigated the feasibility of using computer vision techniques to accurately predict the vocal tract transer function coefficients from vocal tract MRI images. I constructed a dataset by employing linear predictive coding
to map 2D Sagittal-view MRI frames to vocal tract transfer function coefficients. Subsequently, I developed and evaluate a CNN-RNN architecture trained on these MRI frame and coefficient vector pairs. While my architecture tends to predict the mean coefficients of the datasets, I demonstrate the potential for added generalization capabilities provided by a combined CNN-RNN architecture, as well as the ability to learn meaningful representations for understanding speech production mechanisms through gradCAM activation visualizations.&lt;/p>
&lt;figure>
&lt;img src="CVMethodology.png" alt="Methodology Diagram">
&lt;figcaption>Overview of methods used to predict vocal tract transfer function from 2D vocal tract MRI video.Video frames and audio are
separated, and Linear prediction (LP) coefficients are obtained on the audio signal associated with each frame. A Resnet50 CNN is then
fine tuned for a linear regression task on the LP coefficients using the normalized MRI video frames as inputs. This tuned CNN is then
used to train a CNN-LSTM model for higher accuracy prediction of the LP coefficients, which can then be used to resynthesize the original
audio signal associated with a set of MRI image frames&lt;/figcaption>
&lt;/figure>
&lt;p>I implemented a CNN-RNN architecture for a regression
task on estimated vocal tract transfer function, leveraging
MRI images as inputs. The MRI images and vocal tract transfer functions were obtained through the processing of
a multi-speaker dataset of real-time speech production MRI
video. Initially, I fine-tuned a ResNet50 architecture
to predict the vocal tract transfer function coefficients cor
responding to each frame, thereby learning a 256-element
embedding of MRI image features. Subsequently, a recur
rent neural network with LSTM layers was employed. Its
primary objective was to ingest a sequence of MRI frames
and capture temporal patterns in the features derived from
our pretrained CNN, enabling the prediction of LP coeffi
cients for the last time step in the sequence.&lt;/p>
&lt;h2 id="results-and-evaluation">Results and Evaluation&lt;/h2>
&lt;p>While initial visual inspection of predicted frequency responses looked promising, upon closer inspection it became clear that the model was learning and predicting the mean of the dataset.&lt;/p>
&lt;figure>
&lt;img src="pred1.png" alt="pred1">
&lt;img src="pred2.png" alt="pred2">
&lt;figcaption>Frequency responses for predicted and ground truth LP
coefficient vectors. The plot on the left shows an occasion where
our prediction happens to match the label quite well, while the
right plot hints at model prediction of the mean&lt;/figcaption>
&lt;/figure>
&lt;p>The means and variances of the predicted coefficients as compared to the actual coefficients further confirmed this suspicion.&lt;/p>
&lt;!-- &lt;figure>
&lt;img src="./../cvfinalproj/means.png" alt="means">
&lt;img src="./../cvfinalproj/variance.png" alt="variance">
&lt;figcaption>Means and Varaince for each of the 19 LP coefficients across outputs for the test set&lt;/figcaption>
&lt;/figure> -->
&lt;p>When plotting gradCAM activations for the 19 different coefficients, the model seemed to be using anatomically significant regions of images for prediction, for example the throat, soft palate, chin, and lips/nose. This indicates that future work could develop interpretable models which provide valuable insight into the anatomical features crucial for speech production.&lt;/p>
&lt;figure>
&lt;img src="gradcam1.png" alt="gradcam1">
&lt;img src="gradcam2.png" alt="gradcam2">
&lt;figcaption>GradCam activations highlighting the lips, nose, chin, throat, and softpalate for coefficients 1, 3, 4, and 8&lt;/figcaption>
&lt;/figure></description></item><item><title>Membranas</title><link>https://example.com/project/membranas/</link><pubDate>Sun, 01 May 2022 00:00:00 +0000</pubDate><guid>https://example.com/project/membranas/</guid><description>&lt;p>&lt;em>Membranas&lt;/em> is a spatial sound sculpture meant to facilitate more than human exchanges.I worked on this project in collaboration with Nicole L&amp;rsquo;Huillier for her thesis project at the MIT Media Lab. With this work we aimed to explore sounds and vibrations as media for stimulating collectivity, embodied learning, and interconnectivity well beyond the human.&lt;/p>
&lt;div>
&lt;img src="membrana_2.jpg" alt="Membrana 1">
&lt;/div>
&lt;p>When I started working with Nicole on this project, we talked about using an element of the sculpture to record the wind and present it in a sonically enhanced yet authentic way. Wind sort of takes on the sound of whatever its interacting with, and it seemed like the perfect medium to help us facilitate a conversation between people and the environment. It was important to us that this communication occured in the &lt;strong>&lt;em>wind&amp;rsquo;s language&lt;/em>&lt;/strong>, that is, in a way that felt true to the wind&amp;rsquo;s voice.&lt;/p>
&lt;p>Nicole made an initial attempt at this using contact microphones in a silicone membrane. We listened to the audio recorded using these microphones together and determined that it was picking up wind in the way lots of other microphones do: as a sort of &amp;ldquo;white noise&amp;rdquo; that doesn&amp;rsquo;t have much character or liveliness in it. Additionally, the contact microphones recorded a lot of things that weren&amp;rsquo;t the wind, like birds chirping or lawn mowers. We thought we could do better.&lt;/p>
&lt;p>I ran a spectrogram analysis and used that to filter the audio from the contact microphones in a variety of ways. I noticed that a lot of the sound we were getting from the wind was both low in frequency and amplitude. After doing some research, I found some interesting papers about accelerometers being used to record low frequency and low amplitude sound, such as that of ants marching under the ground. This matched the character of the sound we were hoping to record, so we moved forward with the accelerometer as our microphone.&lt;/p>
&lt;p>In order to turn the accelerometer data into digital audio, I created the following data processing pipeline:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>I wrote C++ code to collect about 30 seconds worth of samples from the Accelerometer using an Arduino and the I2C communication protocol.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>In the same C++ code, I wrote a TCP communication protocol to send the samples over WiFi to a computer for post processing.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Finally. I wrote a Python script to estimate the sampling rate of the accelerometer, scale the received values so that they would be audible, and write the processed data to a WAV file to be sent to different speakers in the sculpture.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;div>
&lt;img src="accel_process.png" alt="Accel Process">
&lt;/div>
&lt;p>I learned so so much from working on this project, but the biggest takeaways were:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>How to make communication protocols robust by adding checks for missing data and waiting for missing accelerometer measures (that either got lost in I2C communication with the accelerometer or in TCP communication over WiFi with the computer).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>How to interface with hardware components and microcontrollers, as we tried lots of different accelerometers and boards throughout our experiments.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Learning about filtering methods and signal processing techniques, especially as they relate to digital audio.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Here are some pictures/videos from the finished sound sculpture! I want to thank Nicole so much for her support on the project, and also give a shoutout to Jessie Mindel for the work she did on the sound design and routing for the system!&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://player.vimeo.com/video/818163062" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="vimeo video" webkitallowfullscreen mozallowfullscreen allowfullscreen>&lt;/iframe>
&lt;/div>
&lt;div>
&lt;img src="membrana_1.jpg" alt="Membrana 1">
&lt;/div>
&lt;p>You can check out more of Nicole&amp;rsquo;s work &lt;a href="https://nicolelhuillier.com/" target="_blank" rel="noopener">here&lt;/a>!&lt;/p></description></item><item><title>For Them</title><link>https://example.com/project/forthem/</link><pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate><guid>https://example.com/project/forthem/</guid><description>&lt;p>For Them is a piece for solo clarinet and live electronics that I composed using MaxMSP and sibelius. The piece is meant to be performed with a live solo clarinet, and either a pedal or person that sends a signal to a Max patch to progress the electronics through each phase of the piece (of which there are 8). In this way, the electronics become their own instrument used to accompany the clarinetist. I created the signal routing and processing pipelines for this piece in Max with the help of the &lt;a href=https://cycling74.com/articles/cnmat-odot-tools-for-osc-and-beyond>odot package&lt;/a>.&lt;/p>
&lt;p>For this project, I really wanted to explore sonification and how data can be represented with music in a meaningful way. My piece explores the sensitive but increasingly pertinent topic of gun violence in the United States. To compose the clarinet music, I mapped the timeline of mass shootings in the United States for the past 100 years to a 33 bar piece of music. So every measure between bar 11 and bar 44 represents 3.3 years in United States history.&lt;/p>
&lt;p>I used incidents considered mass shootings by at least two of the following sources: Stanford MSA Data Project, Gun Violence Archive, Mother Jones, The Washington Post, ABC News,FBI, Congressional Research service. I repeat a somber theme over these bars, disrupting the theme with random notes from the 12 semitones. In this way, I also encapsulated some serialist composition techniques.&lt;/p>
&lt;figure>
&lt;img src="for_them_theme.png" alt="Theme">
&lt;figcaption>The opening and main theme of "For Them"&lt;/figcaption>
&lt;/figure>
&lt;figure>
&lt;img src="for_them_perturbs.png" alt="Perturbs">
&lt;figcaption>Perturbations of the opening theme, each representing a mass shooting.&lt;/figcaption>
&lt;/figure>
&lt;p>I wanted to incorporate electronic techniques that would enhance these disruptions, and ended up using reverb and downsampling as the main tools for this.&lt;/p>
&lt;p>Eventually it just becomes too difficult to fit all of the notes in one measure and have the music playable on clarinet. So at measure 40, which would begin to represent the last decade or so, I give the clarinet straight sixteenth notes and have an fm synthesized drunken walk accompany the clarinet. The intention of this was to create harmonic chaos that leads all the way up to the end of measure 44.&lt;/p>
&lt;p>The opening theme also features a poem by John Keats, titled “When I Have Fears that I May Cease to Be”. I wanted to play around with turning speech into music, and when I heard this poem I thought it would be perfect to accompany the clarinet in the beginning. To do this, I first used the ~bonk object on the poem to create a binary signal stream that is &amp;ldquo;on&amp;rdquo; whenever John Keats is speaking. This signal then triggers an fm synthesis patch that picks a random harmonization of several pitches to play. These pitches are randomly chosen from a probability distribution that I have set in Max.&lt;/p>
&lt;figure>
&lt;img src="bonk_workflow.png" alt="Bonk Workflow">
&lt;figcaption>Patch showing Bonk to Fm Synthesis workflow&lt;/figcaption>
&lt;/figure>
&lt;p>The end of the piece features a harmonization of the theme and is mostly a representation of my own frustration and exhaustion with gun violence. I tried to communicate these two emotions with a lax tempo, glissandos, trills, and the minor third harmonizations.&lt;/p>
&lt;figure>
&lt;img src="for_them_trem.png" alt="Harmonizations and Tremolos">
&lt;figcaption>Glissandos, trills, harmonizations&lt;/figcaption>
&lt;/figure>
&lt;p>I was incredibly grateful to have For Them performed by principal clarinetist of the Boston Philharmonic Rane Moore as a final project presentation. Here&amp;rsquo;s a recording of that performance:&lt;/p>
&lt;audio controls>
&lt;source src="for_them_audio.wav" type="audio/wav" />
&lt;p>
Download &lt;a href="for_them_audio.wav">WAV&lt;/a> audio.
&lt;/p>
&lt;/audio>
&lt;p>And the clarinet sheet music can be downloaded at the pdf link above.&lt;/p></description></item></channel></rss>